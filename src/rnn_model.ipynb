{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.finance_metrics import annualized_return\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array \n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import GRU, SimpleRNN, Dense, Dropout, Reshape, Embedding\n",
    "from tensorflow.keras.metrics import MeanSquaredError, RootMeanSquaredError\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RNN():\n",
    "    # Generate the model architecture\n",
    "    rnn_model = Sequential()\n",
    "#     rnn_model.add(Reshape((21,44), input_shape=(44,)))\n",
    "#     rnn_model.add(GRU(128, input_shape=(1,44), dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "#     rnn_model.add(Embedding())\n",
    "    rnn_model.add(GRU(128, input_shape=(21,44), dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "    #input_shape = [X_train.shape[1], X_train.shape[2]]\n",
    "    rnn_model.add(SimpleRNN((128)))\n",
    "    rnn_model.add(Dense(128, activation='relu'))\n",
    "    rnn_model.add(Dropout(0.3))\n",
    "    rnn_model.add(Dense(64, activation='relu'))\n",
    "    rnn_model.add(Dropout(0.3))\n",
    "    rnn_model.add(Dense(32, activation='relu'))\n",
    "    rnn_model.add(Dropout(0.3))\n",
    "    rnn_model.add(Dense(16, activation='relu'))\n",
    "    rnn_model.add(Dropout(0.3))\n",
    "    rnn_model.add(Dense(1))\n",
    "    # Compile the model\n",
    "    rnn_model.compile(optimizer='adam', loss='mean_squared_error',\n",
    "                  metrics=[MeanSquaredError(), RootMeanSquaredError()])\n",
    "    return rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D input\n",
    "def create_dataset (X, y, time_steps = 1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X)-time_steps):\n",
    "        v = X[i:i+time_steps, :]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i+time_steps])\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'data/processed'\n",
    "data_filenames = os.listdir(data_path)\n",
    "\n",
    "models = {}\n",
    "feature_transformers = {}\n",
    "feature_scalers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ADX', 'TY')\n",
      "==================================================\n",
      "Results for pair ('ADX', 'TY'):\n",
      "- Return Difference Stats:\n",
      "  - mean: -0.000025\n",
      "  - std: 0.005070\n",
      "  - quantiles [0.00, 0.25, 0.50, 0.75, 1.00]:\n",
      "    [-0.066671, -0.002313, 6e-06, 0.002432, 0.056949]\n",
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "- Model: Recurrent Neural Net\n",
      "  - MSE: 0.000123\n",
      "  - MAE: 0.008838\n",
      "--------------------------------------------------\n",
      "('AEG', 'ING')\n",
      "('AMAT', 'KLAC')\n",
      "('APA', 'DVN')\n",
      "('ARW', 'AVT')\n",
      "('ASA', 'AU')\n",
      "('AVB', 'EQR')\n",
      "('BAC', 'WFC')\n",
      "('BBVA', 'SAN')\n",
      "('BEN', 'TROW')\n",
      "('BK', 'NTRS')\n",
      "('BMO', 'RY')\n",
      "('BXP', 'VNO')\n",
      "('CCL', 'RCL')\n",
      "('CM', 'TD')\n",
      "('CMA', 'TFC')\n",
      "('COP', 'PEO')\n",
      "('CPT', 'UDR')\n",
      "('CSX', 'NSC')\n",
      "('CUZ', 'WRE')\n",
      "('CVX', 'XOM')\n",
      "('DHI', 'LEN')\n",
      "('DRE', 'PLD')\n",
      "('E', 'TOT')\n",
      "('ED', 'SO')\n",
      "('ELS', 'MAA')\n",
      "('FITB', 'RF')\n",
      "('FRT', 'REG')\n",
      "('FULT', 'VLY')\n",
      "('GAM', 'USA')\n",
      "('GFI', 'HMY')\n",
      "('GOLD', 'NEM')\n",
      "('HAL', 'SLB')\n",
      "('HES', 'OXY')\n",
      "('HIW', 'KRC')\n",
      "('HP', 'PTEN')\n",
      "('HQH', 'HQL')\n",
      "('IAC', 'MTCH')\n",
      "('IFN', 'IIF')\n",
      "('KBH', 'PHM')\n",
      "('KIM', 'WRI')\n",
      "('LSI', 'PSA')\n",
      "('MAC', 'SPG')\n",
      "('MRO', 'MUR')\n",
      "('NNN', 'O')\n",
      "==================================================\n",
      "Results for pair ('NNN', 'O'):\n",
      "- Return Difference Stats:\n",
      "  - mean: -0.000114\n",
      "  - std: 0.008882\n",
      "  - quantiles [0.00, 0.25, 0.50, 0.75, 1.00]:\n",
      "    [-0.095508, -0.004185, -0.000165, 0.004005, 0.0775]\n",
      "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "- Model: Recurrent Neural Net\n",
      "  - MSE: 0.000180\n",
      "  - MAE: 0.008235\n",
      "--------------------------------------------------\n",
      "('PEAK', 'WELL')\n",
      "('PNC', 'USB')\n",
      "('RMT', 'RVT')\n",
      "('TDS', 'USM')\n"
     ]
    }
   ],
   "source": [
    "for data_file in data_filenames:\n",
    "    pairs = data_file[:-4].split('-')\n",
    "    pair = (pairs[0], pairs[1])\n",
    "    print(pair)\n",
    "    \n",
    "    if pair not in [('ADX', 'TY'), ('NNN', 'O')]:\n",
    "        continue\n",
    "\n",
    "    pair_df = pd.read_pickle(os.path.join(data_path, data_file))\n",
    "    X_train = pair_df.loc[\"2000-01-01\":\"2014-12-31\", :].drop(columns=\"Return Diff (t+1)\")\n",
    "    y_train = pair_df.loc[\"2000-01-01\":\"2014-12-31\", [\"Return Diff (t+1)\"]]\n",
    "    X_test = pair_df.loc[\"2015-01-01\":, :].drop(columns=\"Return Diff (t+1)\")\n",
    "    y_test = pair_df.loc[\"2015-01-01\":, [\"Return Diff (t+1)\"]]\n",
    "\n",
    "    # fill invalid values with 1 (for ratios) or 0 (for differences)\n",
    "    X_train[X_train.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_train[y_train.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "    X_test[X_test.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_test[y_test.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "\n",
    "    # add in quantiles as additional feature columns\n",
    "    quantile_transformer = QuantileTransformer()\n",
    "    quantile_transformer.fit(X_train); feature_transformers[pair] = quantile_transformer\n",
    "    X_train.loc[:, [col + \"_QUANTILE\" for col in X_train.columns]] = pd.DataFrame(\n",
    "        pd.DataFrame(quantile_transformer.transform(X_train), index=X_train.index,\n",
    "                     columns=[col + \"_QUANTILE\" for col in X_train.columns]))\n",
    "    X_test.loc[:, [col + \"_QUANTILE\" for col in X_test.columns]] = pd.DataFrame(\n",
    "        pd.DataFrame(quantile_transformer.transform(X_test), index=X_test.index,\n",
    "                     columns=[col + \"_QUANTILE\" for col in X_test.columns]))\n",
    "\n",
    "    # fill invalid values with 1 (for ratios) or 0 (for differences)\n",
    "    X_train[X_train.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_train[y_train.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "    X_test[X_test.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_test[y_test.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "\n",
    "    # scale features\n",
    "    X_scaler = StandardScaler()\n",
    "    X_scaler.fit(X_train); feature_scalers[pair] = X_scaler\n",
    "    X_train = pd.DataFrame(X_scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(X_scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(X_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(X_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(X_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(X_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(y_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(y_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(y_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(y_test))])\n",
    "\n",
    "    split = PredefinedSplit(test_fold=[0 if v else -1 for v in X_train.index < '2012-01-01'])\n",
    "    models[pair] = {\n",
    "#         'Random Forest': GridSearchCV(estimator=RandomForestRegressor(bootstrap=True), param_grid={\n",
    "#             \"n_estimators\": [n for n in range(50, 150, 25)]\n",
    "#         }, cv=split, scoring=\"explained_variance\", n_jobs=-1),\n",
    "#         'Adaptive Boost': GridSearchCV(estimator=AdaBoostRegressor(), param_grid={\n",
    "#             \"n_estimators\": [n for n in range(50, 250, 50)],\n",
    "#             \"loss\": [\"linear\", \"exponential\"]\n",
    "#         }, cv=split, scoring=\"explained_variance\", n_jobs=-1),\n",
    "#         'Gradient Boost': GridSearchCV(estimator=GradientBoostingRegressor(loss=\"huber\"), param_grid={\n",
    "#             \"n_estimators\": [n for n in range(50, 250, 50)]\n",
    "#         }, cv=split, scoring=\"explained_variance\", n_jobs=-1),\n",
    "#         'Neural Net': GridSearchCV(estimator=MLPRegressor(solver=\"lbfgs\", max_iter=1000000), param_grid={\n",
    "#             \"hidden_layer_sizes\": [(250, h1, h2)\n",
    "#                                    for h1 in range(100, 200, 50)\n",
    "#                                    for h2 in range(25, h1//2, 25)]\n",
    "#         }, cv=split, scoring=\"explained_variance\", n_jobs=-1),\n",
    "        'Recurrent Neural Net': KerasRegressor(nb_epoch=5000, batch_size=64, build_fn=build_RNN, verbose=False)\n",
    "    }\n",
    "\n",
    "    print(f\"==================================================\\n\"\n",
    "          f\"Results for pair {pair}:\\n\"\n",
    "          f\"- Return Difference Stats:\\n\"\n",
    "          f\"  - mean: {float(np.mean(y_test)):.06f}\\n\"\n",
    "          f\"  - std: {float(np.std(y_test)):.06f}\\n\"\n",
    "          f\"  - quantiles [0.00, 0.25, 0.50, 0.75, 1.00]:\\n\"\n",
    "          f\"    {[np.round(i, decimals=6) for i in np.quantile(y_test, [0, .25, .50, .75, 1])]}\")\n",
    "    pred_average = None\n",
    "    for model_type, model in models[pair].items():\n",
    "#         print(X_train.shape, np.array(y_train).ravel().shape)\n",
    "        pred = None\n",
    "    \n",
    "        if model_type == \"Recurrent Neural Net\":    \n",
    "            X_train2 = np.array(X_train)\n",
    "            y_train2 = np.array(y_train)\n",
    "            X_test2 = np.array(X_test)\n",
    "            y_test2 = np.array(y_test)\n",
    "\n",
    "            \n",
    "            TIME_STEPS = 21\n",
    "            X_test2, y_test2 = create_dataset(X_test2, y_test2,   \n",
    "                                            TIME_STEPS)\n",
    "            X_train2, y_train2 = create_dataset(X_train2, y_train2, \n",
    "                                              TIME_STEPS)\n",
    "            \n",
    "#             print('X_train.shape: ', X_test.shape)\n",
    "#             print('y_train.shape: ', y_train.shape)\n",
    "#             print('X_test.shape: ', X_test.shape)\n",
    "#             print('y_test.shape: ', y_train.shape)\n",
    "\n",
    "            \n",
    "            model.fit(X_train2, np.array(y_train2).ravel())\n",
    "            pred = model.predict(X_test2)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            model.fit(X_train[:], np.array(y_train[:]).ravel())\n",
    "            pred = model.predict(X_test[:]) \n",
    "            \n",
    "        if model_type != \"Recurrent Neural Net\": \n",
    "            if pred_average is None: pred_average = pred / 4\n",
    "            else: pred_average = pred_average + np.array(pred / 4)\n",
    "        \n",
    "        if model_type != \"Recurrent Neural Net\": \n",
    "            mse = mean_squared_error(np.array(y_test), np.array(pred).ravel())\n",
    "            mae = median_absolute_error(np.array(y_test), np.array(pred).ravel())\n",
    "            print(f\"- Model: {model_type}\\n\"\n",
    "                  f\"  - best parameters: {model.best_params_}\\n\"\n",
    "                  f\"  - MSE: {mse:.06f}\\n\"\n",
    "                  f\"  - MAE: {mae:.06f}\\n\"\n",
    "                  f\"--------------------------------------------------\")\n",
    "        else:\n",
    "            mse = mean_squared_error(np.array(y_test[21:]), np.array(pred).ravel())\n",
    "            mae = median_absolute_error(np.array(y_test[21:]), np.array(pred).ravel())\n",
    "            print(f\"- Model: {model_type}\\n\"\n",
    "                  f\"  - MSE: {mse:.06f}\\n\"\n",
    "                  f\"  - MAE: {mae:.06f}\\n\"\n",
    "                  f\"--------------------------------------------------\")\n",
    "            \n",
    "#     mse = mean_squared_error(np.array(y_test), np.array(pred_average).ravel())\n",
    "#     mae = median_absolute_error(np.array(y_test), np.array(pred_average).ravel())\n",
    "#     print(f\"- Ensemble of Random Forest, Adaptive Boost, Gradient Boost, and Neural Net\\n\"\n",
    "#           f\"  - MSE: {mse:.06f}\\n\"\n",
    "#           f\"  - MAE: {mae:.06f}\\n\"\n",
    "#           f\"--------------------------------------------------\")\n",
    "#     print(f\"==================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "conf_mat_all = None\n",
    "for data_file in data_filenames:\n",
    "    pairs = data_file[:-4].split('-')\n",
    "    pair = (pairs[0], pairs[1])\n",
    "    print(pair)\n",
    "\n",
    "    pair_df = pd.read_pickle(os.path.join(data_path, data_file))\n",
    "    X_train = pair_df.loc[\"2000-01-01\":\"2014-12-31\", :].drop(columns=\"Return Diff (t+1)\")\n",
    "    y_train = pair_df.loc[\"2000-01-01\":\"2014-12-31\", [\"Return Diff (t+1)\"]]\n",
    "    X_test = pair_df.loc[\"2015-01-01\":, :].drop(columns=\"Return Diff (t+1)\")\n",
    "    y_test = pair_df.loc[\"2015-01-01\":, [\"Return Diff (t+1)\"]]\n",
    "\n",
    "    # fill invalid values with 1 (for ratios) or 0 (for differences)\n",
    "    X_train[X_train.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_train[y_train.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "    X_test[X_test.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_test[y_test.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "\n",
    "    # add in quantiles as additional feature columns\n",
    "    quantile_transformer = feature_transformers[pair]\n",
    "    X_train.loc[:, [col + \"_QUANTILE\" for col in X_train.columns]] = pd.DataFrame(\n",
    "        pd.DataFrame(quantile_transformer.transform(X_train), index=X_train.index,\n",
    "                     columns=[col + \"_QUANTILE\" for col in X_train.columns]))\n",
    "    X_test.loc[:, [col + \"_QUANTILE\" for col in X_test.columns]] = pd.DataFrame(\n",
    "        pd.DataFrame(quantile_transformer.transform(X_test), index=X_test.index,\n",
    "                     columns=[col + \"_QUANTILE\" for col in X_test.columns]))\n",
    "\n",
    "    # fill invalid values with 1 (for ratios) or 0 (for differences)\n",
    "    X_train[X_train.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    X_test[X_test.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "\n",
    "    # scale features\n",
    "    X_scaler = feature_scalers[pair]\n",
    "    X_train = pd.DataFrame(X_scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(X_scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(X_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(X_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(X_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(X_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(y_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(y_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(y_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(y_test))])\n",
    "\n",
    "    print(f\"==================================================\\n\"\n",
    "          f\"Results for pair {pair}:\")\n",
    "    pred_ensemble = None\n",
    "    for model_type, model in models[pair].items():\n",
    "        pred = [-1 if i < 0 else 1 for i in np.array(model.predict(X_test)).ravel()]\n",
    "        if pred_ensemble is None: pred_ensemble = np.array(pred) / 4\n",
    "        else: pred_ensemble = pred_ensemble + (np.array(pred) / 4)\n",
    "\n",
    "        conf_mat = confusion_matrix(\n",
    "            [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "            [0 if i < 0 else 1 for i in np.array(pred).ravel()])\n",
    "        accuracy = accuracy_score(\n",
    "            [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "            [0 if i < 0 else 1 for i in np.array(pred).ravel()])\n",
    "        precision = precision_score(\n",
    "            [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "            [0 if i < 0 else 1 for i in np.array(pred).ravel()])\n",
    "        recall = recall_score(\n",
    "            [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "            [0 if i < 0 else 1 for i in np.array(pred).ravel()])\n",
    "        f1 = f1_score(\n",
    "            [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "            [0 if i < 0 else 1 for i in np.array(pred).ravel()])\n",
    "\n",
    "        print(f\"- Model: {model_type}\\n\"\n",
    "              f\"  - Confusion Matrix:\\n{conf_mat}\\n\"\n",
    "              f\"  - Accuracy: {accuracy:.06f}\\n\"\n",
    "              f\"  - Recall: {recall:.06f}\\n\"\n",
    "              f\"  - Precision: {precision:.06f}\\n\"\n",
    "              f\"  - F1-Score: {f1:.06f}\\n\"\n",
    "              f\"--------------------------------------------------\")\n",
    "\n",
    "    conf_mat = confusion_matrix(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "    accuracy = accuracy_score(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "    precision = precision_score(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "    recall = recall_score(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "    f1 = f1_score(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "\n",
    "    if conf_mat_all is None: conf_mat_all = np.array(conf_mat)\n",
    "    else: conf_mat_all = conf_mat_all + np.array(conf_mat)\n",
    "\n",
    "    print(f\"- Ensemble of Random Forest, Adaptive Boost, Gradient Boost, and Neural Net\\n\"\n",
    "          f\"  - Confusion Matrix:\\n{conf_mat}\\n\"\n",
    "          f\"  - Accuracy: {accuracy:.06f}\\n\"\n",
    "          f\"  - Recall: {recall:.06f}\\n\"\n",
    "          f\"  - Precision: {precision:.06f}\\n\"\n",
    "          f\"  - F1-Score: {f1:.06f}\\n\"\n",
    "          f\"--------------------------------------------------\")\n",
    "    print(f\"==================================================\\n\")\n",
    "\n",
    "\n",
    "print(f\"- Confusion Matrix of ensemble prediction across all pairs\\n{conf_mat_all}\\n\"\n",
    "      f\"  - Accuracy: {(conf_mat_all[0][0]+conf_mat_all[1][1]) / np.sum(conf_mat_all):.06f}\\n\"\n",
    "      f\"--------------------------------------------------\")\n",
    "print(f\"==================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def signal_to_multiplier(signal, long):\n",
    "    if           signal == -1.0: return -0.50 * long\n",
    "    elif -1.0 <  signal <= -0.5: return -0.25 * long\n",
    "    elif -0.5 <  signal <= -0.0: return -0.10 * long\n",
    "    elif  0.0 <= signal <   0.5: return  0.10 * long\n",
    "    elif  0.5 <= signal <   1.0: return  0.25 * long\n",
    "    elif         signal ==  1.0: return  0.50 * long\n",
    "    else: raise ValueError(\"Out of range signal provided\")\n",
    "\n",
    "returns = {}\n",
    "for data_file in data_filenames:\n",
    "    pairs = data_file[:-4].split('-')\n",
    "    pair = (pairs[0], pairs[1])\n",
    "    print(pair)\n",
    "\n",
    "    returns[pair] = pd.read_pickle(f\"data/raw/{pair[0]}.zip\").loc[\"2015-01-01\":, [\"Simple Return\"]]\n",
    "    returns[pair][pair[1]] = pd.read_pickle(f\"data/raw/{pair[1]}.zip\").loc[\"2015-01-01\":, [\"Simple Return\"]]\n",
    "    returns[pair].columns = pair\n",
    "\n",
    "    pair_df = pd.read_pickle(os.path.join(data_path, data_file))\n",
    "    X_train = pair_df.loc[\"2000-01-01\":\"2014-12-31\", :].drop(columns=\"Return Diff (t+1)\")\n",
    "    y_train = pair_df.loc[\"2000-01-01\":\"2014-12-31\", [\"Return Diff (t+1)\"]]\n",
    "    X_test = pair_df.loc[\"2015-01-01\":, :].drop(columns=\"Return Diff (t+1)\")\n",
    "    y_test = pair_df.loc[\"2015-01-01\":, [\"Return Diff (t+1)\"]]\n",
    "\n",
    "    # fill invalid values with 1 (for ratios) or 0 (for differences)\n",
    "    X_train[X_train.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_train[y_train.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "    X_test[X_test.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_test[y_test.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "\n",
    "    # add in quantiles as additional feature columns\n",
    "    quantile_transformer = feature_transformers[pair]\n",
    "    X_train.loc[:, [col + \"_QUANTILE\" for col in X_train.columns]] = pd.DataFrame(\n",
    "        pd.DataFrame(quantile_transformer.transform(X_train), index=X_train.index,\n",
    "                     columns=[col + \"_QUANTILE\" for col in X_train.columns]))\n",
    "    X_test.loc[:, [col + \"_QUANTILE\" for col in X_test.columns]] = pd.DataFrame(\n",
    "        pd.DataFrame(quantile_transformer.transform(X_test), index=X_test.index,\n",
    "                     columns=[col + \"_QUANTILE\" for col in X_test.columns]))\n",
    "\n",
    "    # fill invalid values with 1 (for ratios) or 0 (for differences)\n",
    "    X_train[X_train.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_train[y_train.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "    X_test[X_test.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_test[y_test.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "\n",
    "    # scale features and target column\n",
    "    X_scaler = feature_scalers[pair]\n",
    "    X_train = pd.DataFrame(X_scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(X_scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(X_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(X_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(X_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(X_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(y_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(y_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(y_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(y_test))])\n",
    "\n",
    "    print(f\"==================================================\\n\"\n",
    "          f\"Results for pair {pair}:\")\n",
    "    pred_ensemble = None\n",
    "    for model_type, model in models[pair].items():\n",
    "        pred = [-1 if i < 0 else 1 for i in np.array(model.predict(X_test)).ravel()]\n",
    "        if pred_ensemble is None: pred_ensemble = np.array(pred) / 4\n",
    "        else: pred_ensemble = pred_ensemble + (np.array(pred) / 4)\n",
    "        returns[pair][model_type] = pred\n",
    "    returns[pair][\"Ensemble\"] = np.mean(\n",
    "        returns[pair].loc[:, [\"Random Forest\", \"Adaptive Boost\", \"Gradient Boost\", \"Recurrent Neural Net\"]], axis=1)\n",
    "    returns[pair][\"Ensemble\"] = pred_ensemble\n",
    "    returns[pair][pair[0] + \"_SIGNAL\"] = [signal_to_multiplier(sig,  1) for sig in pred_ensemble]\n",
    "    returns[pair][pair[1] + \"_SIGNAL\"] = [signal_to_multiplier(sig, -1) for sig in pred_ensemble]\n",
    "    returns[pair][pair[0] + \"_ADJRET\"] = returns[pair][pair[0]] * returns[pair][pair[0] + \"_SIGNAL\"]\n",
    "    returns[pair][pair[1] + \"_ADJRET\"] = returns[pair][pair[1]] * returns[pair][pair[1] + \"_SIGNAL\"]\n",
    "    returns[pair][\"PAIRS_TRADE_RET\"] = np.mean(returns[pair][[p+\"_ADJRET\" for p in pair]], axis=1)\n",
    "    ann_ret = annualized_return(returns[pair].loc[:, [pair[0],pair[1],\"PAIRS_TRADE_RET\"]])\n",
    "\n",
    "    conf_mat = confusion_matrix(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "    accuracy = accuracy_score(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "    precision = precision_score(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "    recall = recall_score(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "    f1 = f1_score(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "\n",
    "    print(f\"- Ensemble of Random Forest, Adaptive Boost, Gradient Boost, and Recurrent Neural Net\\n\"\n",
    "          f\"  - Confusion Matrix:\\n{conf_mat}\\n\"\n",
    "          f\"  - Accuracy: {accuracy:.06f}\\n\"\n",
    "          f\"  - Recall: {recall:.06f}\\n\"\n",
    "          f\"  - Precision: {precision:.06f}\\n\"\n",
    "          f\"  - F1-Score: {f1:.06f}\\n\"\n",
    "          f\"- Annualized Return:\\n\"\n",
    "          f\"  - {pair[0]}: {ann_ret[pair[0]]:0.06f}\\n\"\n",
    "          f\"  - {pair[1]}: {ann_ret[pair[1]]:0.06f}\\n\"\n",
    "          f\"  - Pairs Trade: {ann_ret['PAIRS_TRADE_RET']:0.06f}\\n\"\n",
    "          f\"--------------------------------------------------\")\n",
    "    print(f\"==================================================\\n\")\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corr = returns[pair].loc[:, [pair[0],pair[1],\"PAIRS_TRADE_RET\"]].corr()\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    heatmap = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0, annot=True,\n",
    "                          square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    heatmap.set_title(f\"{pair[0]}-{pair[1]} Pairs Trade Correlation\")\n",
    "    fig = heatmap.get_figure()\n",
    "    fig.savefig(f\"out/img/{pair[0]}-{pair[1]}-corr-rnn.jpeg\")\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    barplot = ann_ret.plot.bar()\n",
    "    barplot.set_xticklabels(ann_ret.index, rotation=0)\n",
    "    barplot.set_title(f\"{pair[0]}-{pair[1]} Annualized Return\")\n",
    "    fig = barplot.get_figure()\n",
    "    fig.savefig(f\"out/img/{pair[0]}-{pair[1]}-return-rnn.jpeg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
