{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.finance_metrics import annualized_return\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, SimpleRNN, Dense, Dropout\n",
    "from keras.metrics import MeanSquaredError, RootMeanSquaredError\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_RNN():\n",
    "    # Generate the model architecture\n",
    "    rnn_model = Sequential()\n",
    "    rnn_model.add(GRU(128, input_dim=44, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "    rnn_model.add(SimpleRNN(128))\n",
    "    rnn_model.add(Dense(64, activation='relu'))\n",
    "    rnn_model.add(Dropout(0.3))\n",
    "    rnn_model.add(Dense(32, activation='relu'))\n",
    "    rnn_model.add(Dropout(0.3))\n",
    "    rnn_model.add(Dense(16, activation='relu'))\n",
    "    rnn_model.add(Dropout(0.3))\n",
    "    rnn_model.add(Dense(1))\n",
    "    # Compile the model\n",
    "    rnn_model.compile(optimizer='adam', loss='mean_squared_error',\n",
    "                  metrics=[MeanSquaredError(), RootMeanSquaredError()])\n",
    "    return rnn_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_path = r'data/processed'\n",
    "data_filenames = os.listdir(data_path)\n",
    "\n",
    "models = {}\n",
    "feature_transformers = {}\n",
    "feature_scalers = {}\n",
    "for data_file in data_filenames:\n",
    "    pairs = data_file[:-4].split('-')\n",
    "    pair = (pairs[0], pairs[1])\n",
    "    print(pair)\n",
    "\n",
    "    pair_df = pd.read_pickle(os.path.join(data_path, data_file))\n",
    "    X_train = pair_df.loc[\"2000-01-01\":\"2014-12-31\", :].drop(columns=\"Return Diff (t+1)\")\n",
    "    y_train = pair_df.loc[\"2000-01-01\":\"2014-12-31\", [\"Return Diff (t+1)\"]]\n",
    "    X_test = pair_df.loc[\"2015-01-01\":, :].drop(columns=\"Return Diff (t+1)\")\n",
    "    y_test = pair_df.loc[\"2015-01-01\":, [\"Return Diff (t+1)\"]]\n",
    "\n",
    "    # fill invalid values with 1 (for ratios) or 0 (for differences)\n",
    "    X_train[X_train.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_train[y_train.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "    X_test[X_test.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_test[y_test.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "\n",
    "    # add in quantiles as additional feature columns\n",
    "    quantile_transformer = QuantileTransformer()\n",
    "    quantile_transformer.fit(X_train); feature_transformers[pair] = quantile_transformer\n",
    "    X_train.loc[:, [col + \"_QUANTILE\" for col in X_train.columns]] = pd.DataFrame(\n",
    "        pd.DataFrame(quantile_transformer.transform(X_train), index=X_train.index,\n",
    "                     columns=[col + \"_QUANTILE\" for col in X_train.columns]))\n",
    "    X_test.loc[:, [col + \"_QUANTILE\" for col in X_test.columns]] = pd.DataFrame(\n",
    "        pd.DataFrame(quantile_transformer.transform(X_test), index=X_test.index,\n",
    "                     columns=[col + \"_QUANTILE\" for col in X_test.columns]))\n",
    "\n",
    "    # fill invalid values with 1 (for ratios) or 0 (for differences)\n",
    "    X_train[X_train.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_train[y_train.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "    X_test[X_test.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_test[y_test.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "\n",
    "    # scale features\n",
    "    X_scaler = StandardScaler()\n",
    "    X_scaler.fit(X_train); feature_scalers[pair] = X_scaler\n",
    "    X_train = pd.DataFrame(X_scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(X_scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(X_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(X_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(X_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(X_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(y_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(y_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(y_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(y_test))])\n",
    "\n",
    "    split = PredefinedSplit(test_fold=[0 if v else -1 for v in X_train.index < '2012-01-01'])\n",
    "    models[pair] = {\n",
    "        'Random Forest': GridSearchCV(estimator=RandomForestRegressor(bootstrap=True), param_grid={\n",
    "            \"n_estimators\": [n for n in range(50, 150, 25)]\n",
    "        }, cv=split, scoring=\"explained_variance\", n_jobs=-1),\n",
    "        'Adaptive Boost': GridSearchCV(estimator=AdaBoostRegressor(), param_grid={\n",
    "            \"n_estimators\": [n for n in range(50, 250, 50)],\n",
    "            \"loss\": [\"linear\", \"exponential\"]\n",
    "        }, cv=split, scoring=\"explained_variance\", n_jobs=-1),\n",
    "        'Gradient Boost': GridSearchCV(estimator=GradientBoostingRegressor(loss=\"huber\"), param_grid={\n",
    "            \"n_estimators\": [n for n in range(50, 250, 50)]\n",
    "        }, cv=split, scoring=\"explained_variance\", n_jobs=-1),\n",
    "        'Recurrent Neural Net': GridSearchCV(estimator=KerasRegressor(build_fn=build_RNN, verbose=False), param_grid={\n",
    "            \"nb_epoch\": [100, 500, 1000],  # TBD\n",
    "            \"batch_size\": [128, 256]\n",
    "        }, cv=split, scoring=\"explained_variance\", n_jobs=-1)\n",
    "    }\n",
    "\n",
    "    print(f\"==================================================\\n\"\n",
    "          f\"Results for pair {pair}:\\n\"\n",
    "          f\"- Return Difference Stats:\\n\"\n",
    "          f\"  - mean: {float(np.mean(y_test)):.06f}\\n\"\n",
    "          f\"  - std: {float(np.std(y_test)):.06f}\\n\"\n",
    "          f\"  - quantiles [0.00, 0.25, 0.50, 0.75, 1.00]:\\n\"\n",
    "          f\"    {[np.round(i, decimals=6) for i in np.quantile(y_test, [0, .25, .50, .75, 1])]}\")\n",
    "    pred_average = None\n",
    "    for model_type, model in models[pair].items():\n",
    "        model.fit(X_train, np.array(y_train).ravel())\n",
    "        pred = model.predict(X_test)\n",
    "        if pred_average is None: pred_average = pred / 4\n",
    "        else: pred_average = pred_average + np.array(pred / 4)\n",
    "        mse = mean_squared_error(np.array(y_test), np.array(pred).ravel())\n",
    "        mae = median_absolute_error(np.array(y_test), np.array(pred).ravel())\n",
    "        print(f\"- Model: {model_type}\\n\"\n",
    "              f\"  - best parameters: {model.best_params_}\\n\"\n",
    "              f\"  - MSE: {mse:.06f}\\n\"\n",
    "              f\"  - MAE: {mae:.06f}\\n\"\n",
    "              f\"--------------------------------------------------\")\n",
    "    mse = mean_squared_error(np.array(y_test), np.array(pred_average).ravel())\n",
    "    mae = median_absolute_error(np.array(y_test), np.array(pred_average).ravel())\n",
    "    print(f\"- Ensemble of Random Forest, Adaptive Boost, Gradient Boost, and Neural Net\\n\"\n",
    "          f\"  - MSE: {mse:.06f}\\n\"\n",
    "          f\"  - MAE: {mae:.06f}\\n\"\n",
    "          f\"--------------------------------------------------\")\n",
    "    print(f\"==================================================\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conf_mat_all = None\n",
    "for data_file in data_filenames:\n",
    "    pairs = data_file[:-4].split('-')\n",
    "    pair = (pairs[0], pairs[1])\n",
    "    print(pair)\n",
    "\n",
    "    pair_df = pd.read_pickle(os.path.join(data_path, data_file))\n",
    "    X_train = pair_df.loc[\"2000-01-01\":\"2014-12-31\", :].drop(columns=\"Return Diff (t+1)\")\n",
    "    y_train = pair_df.loc[\"2000-01-01\":\"2014-12-31\", [\"Return Diff (t+1)\"]]\n",
    "    X_test = pair_df.loc[\"2015-01-01\":, :].drop(columns=\"Return Diff (t+1)\")\n",
    "    y_test = pair_df.loc[\"2015-01-01\":, [\"Return Diff (t+1)\"]]\n",
    "\n",
    "    # fill invalid values with 1 (for ratios) or 0 (for differences)\n",
    "    X_train[X_train.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_train[y_train.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "    X_test[X_test.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_test[y_test.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "\n",
    "    # add in quantiles as additional feature columns\n",
    "    quantile_transformer = feature_transformers[pair]\n",
    "    X_train.loc[:, [col + \"_QUANTILE\" for col in X_train.columns]] = pd.DataFrame(\n",
    "        pd.DataFrame(quantile_transformer.transform(X_train), index=X_train.index,\n",
    "                     columns=[col + \"_QUANTILE\" for col in X_train.columns]))\n",
    "    X_test.loc[:, [col + \"_QUANTILE\" for col in X_test.columns]] = pd.DataFrame(\n",
    "        pd.DataFrame(quantile_transformer.transform(X_test), index=X_test.index,\n",
    "                     columns=[col + \"_QUANTILE\" for col in X_test.columns]))\n",
    "\n",
    "    # fill invalid values with 1 (for ratios) or 0 (for differences)\n",
    "    X_train[X_train.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    X_test[X_test.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "\n",
    "    # scale features\n",
    "    X_scaler = feature_scalers[pair]\n",
    "    X_train = pd.DataFrame(X_scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(X_scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(X_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(X_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(X_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(X_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(y_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(y_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(y_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(y_test))])\n",
    "\n",
    "    print(f\"==================================================\\n\"\n",
    "          f\"Results for pair {pair}:\")\n",
    "    pred_ensemble = None\n",
    "    for model_type, model in models[pair].items():\n",
    "        pred = [-1 if i < 0 else 1 for i in np.array(model.predict(X_test)).ravel()]\n",
    "        if pred_ensemble is None: pred_ensemble = np.array(pred) / 4\n",
    "        else: pred_ensemble = pred_ensemble + (np.array(pred) / 4)\n",
    "\n",
    "        conf_mat = confusion_matrix(\n",
    "            [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "            [0 if i < 0 else 1 for i in np.array(pred).ravel()])\n",
    "        accuracy = accuracy_score(\n",
    "            [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "            [0 if i < 0 else 1 for i in np.array(pred).ravel()])\n",
    "        precision = precision_score(\n",
    "            [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "            [0 if i < 0 else 1 for i in np.array(pred).ravel()])\n",
    "        recall = recall_score(\n",
    "            [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "            [0 if i < 0 else 1 for i in np.array(pred).ravel()])\n",
    "        f1 = f1_score(\n",
    "            [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "            [0 if i < 0 else 1 for i in np.array(pred).ravel()])\n",
    "\n",
    "        print(f\"- Model: {model_type}\\n\"\n",
    "              f\"  - Confusion Matrix:\\n{conf_mat}\\n\"\n",
    "              f\"  - Accuracy: {accuracy:.06f}\\n\"\n",
    "              f\"  - Recall: {recall:.06f}\\n\"\n",
    "              f\"  - Precision: {precision:.06f}\\n\"\n",
    "              f\"  - F1-Score: {f1:.06f}\\n\"\n",
    "              f\"--------------------------------------------------\")\n",
    "\n",
    "    conf_mat = confusion_matrix(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "    accuracy = accuracy_score(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "    precision = precision_score(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "    recall = recall_score(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "    f1 = f1_score(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "\n",
    "    if conf_mat_all is None: conf_mat_all = np.array(conf_mat)\n",
    "    else: conf_mat_all = conf_mat_all + np.array(conf_mat)\n",
    "\n",
    "    print(f\"- Ensemble of Random Forest, Adaptive Boost, Gradient Boost, and Neural Net\\n\"\n",
    "          f\"  - Confusion Matrix:\\n{conf_mat}\\n\"\n",
    "          f\"  - Accuracy: {accuracy:.06f}\\n\"\n",
    "          f\"  - Recall: {recall:.06f}\\n\"\n",
    "          f\"  - Precision: {precision:.06f}\\n\"\n",
    "          f\"  - F1-Score: {f1:.06f}\\n\"\n",
    "          f\"--------------------------------------------------\")\n",
    "    print(f\"==================================================\\n\")\n",
    "\n",
    "\n",
    "print(f\"- Confusion Matrix of ensemble prediction across all pairs\\n{conf_mat_all}\\n\"\n",
    "      f\"  - Accuracy: {(conf_mat_all[0][0]+conf_mat_all[1][1]) / np.sum(conf_mat_all):.06f}\\n\"\n",
    "      f\"--------------------------------------------------\")\n",
    "print(f\"==================================================\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def signal_to_multiplier(signal, long):\n",
    "    if           signal == -1.0: return -0.50 * long\n",
    "    elif -1.0 <  signal <= -0.5: return -0.25 * long\n",
    "    elif -0.5 <  signal <= -0.0: return -0.10 * long\n",
    "    elif  0.0 <= signal <   0.5: return  0.10 * long\n",
    "    elif  0.5 <= signal <   1.0: return  0.25 * long\n",
    "    elif         signal ==  1.0: return  0.50 * long\n",
    "    else: raise ValueError(\"Out of range signal provided\")\n",
    "\n",
    "returns = {}\n",
    "for data_file in data_filenames:\n",
    "    pairs = data_file[:-4].split('-')\n",
    "    pair = (pairs[0], pairs[1])\n",
    "    print(pair)\n",
    "\n",
    "    returns[pair] = pd.read_pickle(f\"data/raw/{pair[0]}.zip\").loc[\"2015-01-01\":, [\"Simple Return\"]]\n",
    "    returns[pair][pair[1]] = pd.read_pickle(f\"data/raw/{pair[1]}.zip\").loc[\"2015-01-01\":, [\"Simple Return\"]]\n",
    "    returns[pair].columns = pair\n",
    "\n",
    "    pair_df = pd.read_pickle(os.path.join(data_path, data_file))\n",
    "    X_train = pair_df.loc[\"2000-01-01\":\"2014-12-31\", :].drop(columns=\"Return Diff (t+1)\")\n",
    "    y_train = pair_df.loc[\"2000-01-01\":\"2014-12-31\", [\"Return Diff (t+1)\"]]\n",
    "    X_test = pair_df.loc[\"2015-01-01\":, :].drop(columns=\"Return Diff (t+1)\")\n",
    "    y_test = pair_df.loc[\"2015-01-01\":, [\"Return Diff (t+1)\"]]\n",
    "\n",
    "    # fill invalid values with 1 (for ratios) or 0 (for differences)\n",
    "    X_train[X_train.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_train[y_train.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "    X_test[X_test.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_test[y_test.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "\n",
    "    # add in quantiles as additional feature columns\n",
    "    quantile_transformer = feature_transformers[pair]\n",
    "    X_train.loc[:, [col + \"_QUANTILE\" for col in X_train.columns]] = pd.DataFrame(\n",
    "        pd.DataFrame(quantile_transformer.transform(X_train), index=X_train.index,\n",
    "                     columns=[col + \"_QUANTILE\" for col in X_train.columns]))\n",
    "    X_test.loc[:, [col + \"_QUANTILE\" for col in X_test.columns]] = pd.DataFrame(\n",
    "        pd.DataFrame(quantile_transformer.transform(X_test), index=X_test.index,\n",
    "                     columns=[col + \"_QUANTILE\" for col in X_test.columns]))\n",
    "\n",
    "    # fill invalid values with 1 (for ratios) or 0 (for differences)\n",
    "    X_train[X_train.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_train[y_train.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "    X_test[X_test.isin([-np.inf, np.inf, np.nan])] = 1\n",
    "    y_test[y_test.isin([-np.inf, np.inf, np.nan])] = 0\n",
    "\n",
    "    # scale features and target column\n",
    "    X_scaler = feature_scalers[pair]\n",
    "    X_train = pd.DataFrame(X_scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(X_scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(X_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(X_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(X_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(X_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(y_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(y_train))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isinf(y_test))])\n",
    "    assert not any([any(arr) for arr in np.array(np.isnan(y_test))])\n",
    "\n",
    "    print(f\"==================================================\\n\"\n",
    "          f\"Results for pair {pair}:\")\n",
    "    pred_ensemble = None\n",
    "    for model_type, model in models[pair].items():\n",
    "        pred = [-1 if i < 0 else 1 for i in np.array(model.predict(X_test)).ravel()]\n",
    "        if pred_ensemble is None: pred_ensemble = np.array(pred) / 4\n",
    "        else: pred_ensemble = pred_ensemble + (np.array(pred) / 4)\n",
    "        returns[pair][model_type] = pred\n",
    "    returns[pair][\"Ensemble\"] = np.mean(\n",
    "        returns[pair].loc[:, [\"Random Forest\", \"Adaptive Boost\", \"Gradient Boost\", \"Recurrent Neural Net\"]], axis=1)\n",
    "    returns[pair][\"Ensemble\"] = pred_ensemble\n",
    "    returns[pair][pair[0] + \"_SIGNAL\"] = [signal_to_multiplier(sig,  1) for sig in pred_ensemble]\n",
    "    returns[pair][pair[1] + \"_SIGNAL\"] = [signal_to_multiplier(sig, -1) for sig in pred_ensemble]\n",
    "    returns[pair][pair[0] + \"_ADJRET\"] = returns[pair][pair[0]] * returns[pair][pair[0] + \"_SIGNAL\"]\n",
    "    returns[pair][pair[1] + \"_ADJRET\"] = returns[pair][pair[1]] * returns[pair][pair[1] + \"_SIGNAL\"]\n",
    "    returns[pair][\"PAIRS_TRADE_RET\"] = np.mean(returns[pair][[p+\"_ADJRET\" for p in pair]], axis=1)\n",
    "    ann_ret = annualized_return(returns[pair].loc[:, [pair[0],pair[1],\"PAIRS_TRADE_RET\"]])\n",
    "\n",
    "    conf_mat = confusion_matrix(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "    accuracy = accuracy_score(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "    precision = precision_score(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "    recall = recall_score(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "    f1 = f1_score(\n",
    "        [0 if i < 0 else 1 for i in y_test.to_numpy().ravel()],\n",
    "        [0 if i < 0 else 1 for i in np.array(pred_ensemble).ravel()])\n",
    "\n",
    "    print(f\"- Ensemble of Random Forest, Adaptive Boost, Gradient Boost, and Recurrent Neural Net\\n\"\n",
    "          f\"  - Confusion Matrix:\\n{conf_mat}\\n\"\n",
    "          f\"  - Accuracy: {accuracy:.06f}\\n\"\n",
    "          f\"  - Recall: {recall:.06f}\\n\"\n",
    "          f\"  - Precision: {precision:.06f}\\n\"\n",
    "          f\"  - F1-Score: {f1:.06f}\\n\"\n",
    "          f\"- Annualized Return:\\n\"\n",
    "          f\"  - {pair[0]}: {ann_ret[pair[0]]:0.06f}\\n\"\n",
    "          f\"  - {pair[1]}: {ann_ret[pair[1]]:0.06f}\\n\"\n",
    "          f\"  - Pairs Trade: {ann_ret['PAIRS_TRADE_RET']:0.06f}\\n\"\n",
    "          f\"--------------------------------------------------\")\n",
    "    print(f\"==================================================\\n\")\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corr = returns[pair].loc[:, [pair[0],pair[1],\"PAIRS_TRADE_RET\"]].corr()\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    heatmap = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0, annot=True,\n",
    "                          square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    heatmap.set_title(f\"{pair[0]}-{pair[1]} Pairs Trade Correlation\")\n",
    "    fig = heatmap.get_figure()\n",
    "    fig.savefig(f\"out/img/{pair[0]}-{pair[1]}-corr-rnn.jpeg\")\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    barplot = ann_ret.plot.bar()\n",
    "    barplot.set_xticklabels(ann_ret.index, rotation=0)\n",
    "    barplot.set_title(f\"{pair[0]}-{pair[1]} Annualized Return\")\n",
    "    fig = barplot.get_figure()\n",
    "    fig.savefig(f\"out/img/{pair[0]}-{pair[1]}-return-rnn.jpeg\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}